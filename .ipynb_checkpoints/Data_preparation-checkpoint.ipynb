{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5404068-67c1-431c-8cba-f9c44996691d",
   "metadata": {},
   "source": [
    "# Creating dataset from LIDC-IDRI scans "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4402b-60ea-4650-9f5b-cf6cf2e8deac",
   "metadata": {},
   "source": [
    "The scope of this notebok is to prepare dataset for future training of model. I follow code published in the ConRad repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596b73c-4d61-4630-bc9a-c9f84ee2a23c",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d2d7ea-070f-4491-bcba-fd127d0f98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pylidc as pl\n",
    "from pylidc.utils import consensus\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "import sys, os\n",
    "import ctypes\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc8937-e75f-4a5c-964a-9118d3d37b14",
   "metadata": {},
   "source": [
    "## Creating pylidcrc file\n",
    "Pylidc library needs a configurational file with the defined path to the LIDC-IDRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "849c5023-91e8-40b4-8467-b7d6050ade6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../LIDC_sets/manifest_10_patients/LIDC-IDRI'\n",
    "f = open('/home/dzban112/.pylidcrc', 'w') # mode 'w' clear file and starts writing from the begining.\n",
    "f.write(f'[dicom]\\npath = {path}\\n\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8277dfe-d854-45c1-b9dd-ae31eb08957d",
   "metadata": {},
   "source": [
    "Just to check if file was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50cd334c-8a21-4dd6-b767-88ecef8caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dicom]\n",
      "path = ../LIDC_sets/manifest_10_patients/LIDC-IDRI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! cat /home/dzban112/.pylidcrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ab3f5-26b6-4b51-b675-9ac10dbfcd1f",
   "metadata": {},
   "source": [
    "## Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "677c699d-373f-4bb8-ab37-3447c5b71fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to LIDC-IDRI data\n",
    "base_path = \"../LIDC_sets/manifest_10_patients/LIDC-IDRI\"\n",
    "# define path to save the dataset\n",
    "save_path = \"./dataset\"\n",
    "\n",
    "h= 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e03406-e33d-4d3b-80fc-29247f4fac23",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4377ad3d-c50a-4d4c-82a8-8898b3e68fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is only needed to catch the warning from torchio, samples with a warning are excluded from the dataset\n",
    "def flush(stream):\n",
    "    try:\n",
    "        ctypes.libc.fflush(None)\n",
    "        stream.flush()\n",
    "    except (AttributeError, ValueError, IOError):\n",
    "        pass  # unsupported\n",
    "def fileno(file_or_fd):\n",
    "    print(\"***\")\n",
    "    #fd = getattr(file_or_fd, \"fileno\", lambda: file_or_fd)() ### There is a problem!\n",
    "    fd = file_or_fd.fileno()\n",
    "    print(fd)\n",
    "    if not isinstance(fd, int):\n",
    "        raise ValueError(\"Expected a file (`.fileno()`) or a file descriptor\")\n",
    "    return fd\n",
    "@contextmanager\n",
    "def stdout_redirected(to=os.devnull, stdout=None):\n",
    "    if stdout is None:\n",
    "        stdout = sys.stdout\n",
    "    print(stdout)\n",
    "    stdout_fd = fileno(stdout)\n",
    "\n",
    "    with os.fdopen(os.dup(stdout_fd), \"wb\") as copied:\n",
    "        flush(stdout)\n",
    "        try:\n",
    "            os.dup2(fileno(to), stdout_fd)  # $ exec >&to\n",
    "        except ValueError:  # filename\n",
    "            with open(to, \"wb\") as to_file:\n",
    "                os.dup2(to_file.fileno(), stdout_fd)  # $ exec > to\n",
    "        try:\n",
    "            yield stdout  # allow code to be run with the redirected stdout\n",
    "        finally:\n",
    "            flush(stdout)\n",
    "            os.dup2(copied.fileno(), stdout_fd)  # $ exec >&copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921172c-a1d5-4788-90e5-c7a05e351fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf738e86-7f22-4b65-9d6f-186707ff2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract slices of dimension h x h x h around nodule center \n",
    "# and the corresponding segmentations\n",
    "def process_nodule_vol(nodule, h=32):\n",
    "    # Below, there is extracted path do the dicom files for FIRST annotation for each nodule. WHY first?\n",
    "    dicom = nodule[0].scan.get_path_to_dicom_files()\n",
    "    # Median of annotated malignancy. Each nodule may has few annotation and few annotated malignancy values.\n",
    "    median_malig = np.median([ann.malignancy for ann in nodule])\n",
    "    \n",
    "#     # catch errors from torchio and throw an exception so that this nodule is skipped\n",
    "    with open(\"output.txt\", \"w\") as f, stdout_redirected(f, stdout=sys.stderr):\n",
    "        tio_image = tio.ScalarImage(dicom)\n",
    "        spacing = tio_image.spacing\n",
    "        # resample isotropically to 1mm spacing\n",
    "        transform = tio.Resample(1)\n",
    "        res_image = transform(tio_image)\n",
    "        res_data = torch.movedim(res_image.data, (0,1,2,3), (0,2,1,3))\n",
    "        \n",
    "    with open(\"output.txt\") as f:\n",
    "        content = f.read()\n",
    "    if \"warning\" in content.lower():\n",
    "        raise RuntimeError(\"SimpleITK Warning .. skip!\")\n",
    "    open(\"output.txt\", \"w\").close()\n",
    "    \n",
    "    cmask,cbbox,masks = consensus(nodule, clevel=0.5)\n",
    "    \n",
    "    # resample cbbox accordingly\n",
    "    res_cbbox = [(round(cbbox[i].start*spacing[i]), \n",
    "                  round(cbbox[i].stop*spacing[i])) for i in range(3)]\n",
    "    \n",
    "    res_cmask = ndimage.zoom(cmask.astype(int), spacing)\n",
    "    \n",
    "    # center of cbbox\n",
    "    res_cbbox0 = [round((res_cbbox[i][0]+res_cbbox[i][1])/2) for i in range(3)]\n",
    "    \n",
    "    # cmask is given realtive to cbbox, express relative to original volume\n",
    "    g = np.zeros(res_data.shape[1:])\n",
    "    g[res_cbbox[0][0]:res_cbbox[0][0]+res_cmask.shape[0], \n",
    "      res_cbbox[1][0]:res_cbbox[1][0]+res_cmask.shape[1],\n",
    "      res_cbbox[2][0]:res_cbbox[2][0]+res_cmask.shape[2],] = res_cmask\n",
    "\n",
    "    # extract volumes of dimension 2k x 2k x 2k\n",
    "    k = int(h/2)\n",
    "    slices = (\n",
    "                slice(res_cbbox0[0]-k, res_cbbox0[0]+k),\n",
    "                slice(res_cbbox0[1]-k, res_cbbox0[1]+k),\n",
    "                slice(res_cbbox0[2]-k, res_cbbox0[2]+k)\n",
    "             )\n",
    "\n",
    "    crop = res_data[0][slices]\n",
    "\n",
    "    g = torch.tensor(g)\n",
    "    mask = g[slices]\n",
    "    \n",
    "    assert crop.shape == torch.Size([h,h,h])\n",
    "    assert mask.shape == torch.Size([h,h,h])\n",
    "    \n",
    "    return median_malig, crop, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832cd39-0506-4979-b4ea-e58ffef9a1ec",
   "metadata": {},
   "source": [
    "## Extracting nodule volumes, associated tumor masks and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad7d05f8-191e-4040-9319-4bcef11baa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:03,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n",
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:06,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:02<00:03,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n",
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:03<00:04,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:04<00:03,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:05<00:02,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n",
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:08<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n",
      "fileno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e8de3ac92964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{save_path}/annotations_new.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_annotations' is not defined"
     ]
    }
   ],
   "source": [
    "labels = {}\n",
    "new_id = 1\n",
    "match = []\n",
    "#Path object is a useful path object from pathlib library:\n",
    "Path(f\"{save_path}/crops\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{save_path}/masks\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "attributes = [\n",
    "    \"subtlety\",\n",
    "    \"internalStructure\",\n",
    "    \"calcification\",\n",
    "    \"sphericity\",\n",
    "    \"margin\",\n",
    "    \"lobulation\",\n",
    "    \"spiculation\",\n",
    "    \"texture\"\n",
    "]\n",
    "\n",
    "d = glob.glob(f\"{base_path}/*\")\n",
    "d.sort() # alphabetical sorting of files\n",
    "# in that case files will be sorted in according to the patient id:\n",
    "# LIDC-IDRI-0001, LIDC-IDRI-0002 etc.\n",
    "\n",
    "# list of patient ids\n",
    "pids = [elt.split('/')[-1].split('-')[-1] for elt in d]\n",
    "\n",
    "for patient_id in tqdm(pids):\n",
    "    #print(patient_id)\n",
    "    # Selecting from database scan for the patient with given id. Query object needs transformation to the scan object\n",
    "    # what is done by the .first() method.\n",
    "    scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == f\"LIDC-IDRI-{patient_id}\").first()\n",
    "    # clustering annotations. It means assigning annotation to the concrete nodules.\n",
    "    # Annotations are on scan slices, but nodule is a 3D object in lung tissue! So few annotations may be assigned to the one nodule.\n",
    "    nodules = scan.cluster_annotations()\n",
    "    if len(nodules) == 0: # in case, when patient (hopefully) hasn't any nodule.\n",
    "        continue\n",
    "    k = 0\n",
    "    for nodule in nodules:\n",
    "        num_annotations=len(nodule)\n",
    "# in the orignal code they considered only nodules with more than 2 annotations\n",
    "# and nodules with less or equal 4 annotaions. Because they argue that clusterization\n",
    "# is ambiguous when there is more than 4 annotations for a nodule.\n",
    "        if num_annotations > 4:\n",
    "            print(\"skipping!\")  \n",
    "        if (num_annotations > 2 and num_annotations <= 4):\n",
    "            try:\n",
    "                median_malig, crop, mask = process_nodule_vol(nodule, h=h)\n",
    "                print(median_malig)\n",
    "                str_new_id = str(new_id).zfill(4)\n",
    "                append = False\n",
    "                if(median_malig > 3):\n",
    "                    avg_annotations[\"target\"].append(1)\n",
    "                    append = True\n",
    "                    new_id += 1\n",
    "                elif(median_malig < 3):\n",
    "                    avg_annotations[\"target\"].append(0)\n",
    "                    append = True\n",
    "                    new_id += 1\n",
    "                if(append):\n",
    "                    avg_annotations[\"diameter\"].append(np.mean([ann.diameter for ann in nodule]))\n",
    "                    avg_annotations[\"path\"].append(f\"{str_new_id}.pt\")\n",
    "                    for att in attributes:\n",
    "                        avg_annotations[att].append(np.mean([vars(ann)[att] for ann in nodule]))\n",
    "                        \n",
    "                    match.append([patient_id, k, new_id])\n",
    "                    torch.save(crop.clone(), f\"{save_path}/crops/{str_new_id}.pt\")\n",
    "                    torch.save(mask.clone(), f\"{save_path}/masks/{str_new_id}.pt\")\n",
    "            # if creation of crop fails for any reason, skip to next nodule\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        k += 1\n",
    "with open(f\"{save_path}/match.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(match, handle)\n",
    "    \n",
    "with open(f\"{save_path}/annotations_new.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(avg_annotations, handle)\n",
    "os.remove(\"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d87a9fd-9f42-4b67-9556-1c9110a478bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Annotation(id=144,scan_id=21)]\n"
     ]
    }
   ],
   "source": [
    "print(nodules[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43604828-5bd1-4d2b-b055-383cd00e5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## próbna totalnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a32341c-b88c-410a-9e6a-b39374236a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipykernel.iostream.OutStream object at 0x2adf5dfb15c0>\n",
      "***\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-7281b0e614fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_nodule_vol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-4188a0ae1e8a>\u001b[0m in \u001b[0;36mprocess_nodule_vol\u001b[0;34m(nodule, h)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     # catch errors from torchio and throw an exception so that this nodule is skipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout_redirected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtio_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mspacing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtio_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-bfc4f4546e7e>\u001b[0m in \u001b[0;36mstdout_redirected\u001b[0;34m(to, stdout)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstdout_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout_fd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcopied\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-bfc4f4546e7e>\u001b[0m in \u001b[0;36mfileno\u001b[0;34m(file_or_fd)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#fd = getattr(file_or_fd, \"fileno\", lambda: file_or_fd)() ### There is a problem!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_or_fd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "process_nodule_vol(nodules[0], h=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee771a-2574-4d56-96db-c721a3501980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract slices of dimension h x h x h around nodule center \n",
    "# and the corresponding segmentations\n",
    "def process_nodule_vol(nodule, h=32):\n",
    "    # Below, there is extracted path do the dicom files for FIRST annotation for each nodule. WHY first?\n",
    "    dicom = nodule[0].scan.get_path_to_dicom_files()\n",
    "    # Median of annotated malignancy. Each nodule may has few annotation and few annotated malignancy values.\n",
    "    median_malig = np.median([ann.malignancy for ann in nodule])\n",
    "    \n",
    "#     # catch errors from torchio and throw an exception so that this nodule is skipped\n",
    "    with open(\"output.txt\", \"w\") as f, stdout_redirected(f, stdout=sys.stderr):\n",
    "        tio_image = tio.ScalarImage(dicom)\n",
    "        spacing = tio_image.spacing\n",
    "        # resample isotropically to 1mm spacing\n",
    "        transform = tio.Resample(1)\n",
    "        res_image = transform(tio_image)\n",
    "        res_data = torch.movedim(res_image.data, (0,1,2,3), (0,2,1,3))\n",
    "        \n",
    "    with open(\"output.txt\") as f:\n",
    "        content = f.read()\n",
    "    if \"warning\" in content.lower():\n",
    "        raise RuntimeError(\"SimpleITK Warning .. skip!\")\n",
    "    open(\"output.txt\", \"w\").close()\n",
    "    \n",
    "    cmask,cbbox,masks = consensus(nodule, clevel=0.5)\n",
    "    \n",
    "    # resample cbbox accordingly\n",
    "    res_cbbox = [(round(cbbox[i].start*spacing[i]), \n",
    "                  round(cbbox[i].stop*spacing[i])) for i in range(3)]\n",
    "    \n",
    "    res_cmask = ndimage.zoom(cmask.astype(int), spacing)\n",
    "    \n",
    "    # center of cbbox\n",
    "    res_cbbox0 = [round((res_cbbox[i][0]+res_cbbox[i][1])/2) for i in range(3)]\n",
    "    \n",
    "    # cmask is given realtive to cbbox, express relative to original volume\n",
    "    g = np.zeros(res_data.shape[1:])\n",
    "    g[res_cbbox[0][0]:res_cbbox[0][0]+res_cmask.shape[0], \n",
    "      res_cbbox[1][0]:res_cbbox[1][0]+res_cmask.shape[1],\n",
    "      res_cbbox[2][0]:res_cbbox[2][0]+res_cmask.shape[2],] = res_cmask\n",
    "\n",
    "    # extract volumes of dimension 2k x 2k x 2k\n",
    "    k = int(h/2)\n",
    "    slices = (\n",
    "                slice(res_cbbox0[0]-k, res_cbbox0[0]+k),\n",
    "                slice(res_cbbox0[1]-k, res_cbbox0[1]+k),\n",
    "                slice(res_cbbox0[2]-k, res_cbbox0[2]+k)\n",
    "             )\n",
    "\n",
    "    crop = res_data[0][slices]\n",
    "\n",
    "    g = torch.tensor(g)\n",
    "    mask = g[slices]\n",
    "    \n",
    "    assert crop.shape == torch.Size([h,h,h])\n",
    "    assert mask.shape == torch.Size([h,h,h])\n",
    "    \n",
    "    return median_malig, crop, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c886ac7a-e0bf-40b7-ba85-bb6c3290c2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = open('output.txt', 'w') # mode 'w' clear file and starts writing from the begining.\n",
    "g.write(f'[dicom]\\npath = {path}\\n\\n')\n",
    "g.fileno()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4325b79-05a1-475e-a143-a9c2217d2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd853aa-ec5a-4a49-a9d5-fe981d3069c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
